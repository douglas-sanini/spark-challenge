{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "280654e8-e2d8-490e-9df9-bb5cccc36cc7",
   "metadata": {},
   "source": [
    "# Jupyter + Spark + MySQL Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f39a5591-2d18-48ad-9ede-fa7fe1343e45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta-spark\n",
      "  Downloading delta_spark-3.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: pyspark<3.6.0,>=3.5.3 in /opt/conda/lib/python3.11/site-packages (from delta-spark) (3.5.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.17.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark<3.6.0,>=3.5.3->delta-spark) (0.10.9.7)\n",
      "Downloading delta_spark-3.3.0-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: delta-spark\n",
      "Successfully installed delta-spark-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff703af-801d-4e24-8721-a11b181f679b",
   "metadata": {},
   "source": [
    "Initializing a Spark session with the Delta Spark session extension. This configuration is essential for working with Delta Lake tables and will be necessary later when saving tables locally in Delta format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c9567a3-d53c-4f67-b6d8-008a2923bd33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e111d04e-bd36-4851-981c-b41a6a514d25;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.3.0 in central\n",
      "\tfound io.delta#delta-storage;3.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.3.0/delta-spark_2.12-3.3.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-spark_2.12;3.3.0!delta-spark_2.12.jar (1432ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/3.3.0/delta-storage-3.3.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;3.3.0!delta-storage.jar (271ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.9.3!antlr4-runtime.jar (280ms)\n",
      ":: resolution report :: resolve 4277ms :: artifacts dl 1988ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e111d04e-bd36-4851-981c-b41a6a514d25\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (7313kB/15ms)\n",
      "25/02/06 10:51:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2010360-7717-40fc-9fb8-5e724091d4a4",
   "metadata": {},
   "source": [
    "Establishing connection to MySQL database and retrieving a list of all available tables in the `test_db` database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed1fd71c-086b-4321-89bb-ee0795ba8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_url = \"jdbc:mysql://localhost:3306/test_db?useSSL=false&allowPublicKeyRetrieval=true\"\n",
    "\n",
    "database_properties = {\n",
    "    \"user\":  \"jovyan\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9108d51f-9b3c-4185-a506-429366a48e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "| TABLE_NAME|\n",
      "+-----------+\n",
      "|order_items|\n",
      "|     orders|\n",
      "|   products|\n",
      "|      users|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.jdbc(url=database_url, table=\"INFORMATION_SCHEMA.TABLES\", properties=database_properties)\n",
    "df.createOrReplaceTempView(\"tables_metadatas\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT TABLE_NAME \n",
    "    FROM tables_metadatas\n",
    "    WHERE TABLE_SCHEMA = 'test_db'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b84e4-d0d5-498c-ab32-23f32d7c00ab",
   "metadata": {},
   "source": [
    "Creating function that automates the process of:\n",
    "1. **Creating a DataFrame:** Reads data from the specified table.\n",
    "2. **Creating a Temporary View:** Registers the DataFrame as a temporary view for SQL queries.\n",
    "3. **Displaying Table Output:** Shows the contents of the table directly.\n",
    "\n",
    "Simply provide the table name as input to use this function efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d294e815-1f50-45fb-bd78-6d85ac8f84bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_show_view(table):\n",
    "    df =  spark.read.jdbc(url=database_url, table=table, properties=database_properties)\n",
    "    df.createOrReplaceTempView(table)\n",
    "    spark.sql(f\"select * from {table}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9e88d33-b1ce-48c7-8020-b789a010ba79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+--------+\n",
      "| id|order_id|product_id|quantity|\n",
      "+---+--------+----------+--------+\n",
      "|  1|       1|         1|       1|\n",
      "|  2|       1|         3|       2|\n",
      "|  3|       2|         2|       1|\n",
      "|  4|       3|         4|       1|\n",
      "|  5|       4|         5|       3|\n",
      "|  6|       5|         1|       2|\n",
      "|  7|       6|         3|       1|\n",
      "+---+--------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_and_show_view(\"order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea334bbd-00e8-4123-b327-b6b28e1ee24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------------+\n",
      "| id|user_id|order_date|total_amount|\n",
      "+---+-------+----------+------------+\n",
      "|  1|      1|2024-01-10|      100.50|\n",
      "|  2|      1|2024-02-15|      200.75|\n",
      "|  3|      2|2024-01-05|       50.00|\n",
      "|  4|      3|2024-03-01|      150.00|\n",
      "|  5|      4|2024-02-20|      300.99|\n",
      "|  6|      5|2024-01-25|       75.25|\n",
      "+---+-------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_and_show_view(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57c45d16-f487-4fd7-a468-5f49592e6a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+\n",
      "| id|      name| price|\n",
      "+---+----------+------+\n",
      "|  1|    Laptop|999.99|\n",
      "|  2|Smartphone|599.99|\n",
      "|  3|Headphones| 79.99|\n",
      "|  4|  Keyboard| 49.99|\n",
      "|  5|     Mouse| 39.99|\n",
      "+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_and_show_view(\"products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce060b37-ade5-4401-98e8-abfb9863e3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------------------+\n",
      "| id|   name|age|              email|\n",
      "+---+-------+---+-------------------+\n",
      "|  1|  Alice| 30|  alice@example.com|\n",
      "|  2|    Bob| 25|    bob@example.com|\n",
      "|  3|Charlie| 35|charlie@example.com|\n",
      "|  4|  David| 40|  david@example.com|\n",
      "|  5|    Eve| 28|    eve@example.com|\n",
      "+---+-------+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_and_show_view(\"users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea2dc44-a779-458a-bb95-004363320177",
   "metadata": {},
   "source": [
    "Creating query that leverages four interconnected tables to accurately determine how much each user spent. By appropriately joining these tables, we can track user purchases, item quantities, and product prices to calculate total spending per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bba9087-fc29-40cf-aeef-910f5c814e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------------+------------+------------------+-------------+-----------+\n",
      "|   name|order_date|order_item_quantity|product_name|order_total_amount|product_price|total_price|\n",
      "+-------+----------+-------------------+------------+------------------+-------------+-----------+\n",
      "|  Alice|2024-01-10|                  2|  Headphones|            100.50|        79.99|     159.98|\n",
      "|  Alice|2024-01-10|                  1|      Laptop|            100.50|       999.99|     999.99|\n",
      "|  Alice|2024-02-15|                  1|  Smartphone|            200.75|       599.99|     599.99|\n",
      "|    Bob|2024-01-05|                  1|    Keyboard|             50.00|        49.99|      49.99|\n",
      "|Charlie|2024-03-01|                  3|       Mouse|            150.00|        39.99|     119.97|\n",
      "|  David|2024-02-20|                  2|      Laptop|            300.99|       999.99|    1999.98|\n",
      "|    Eve|2024-01-25|                  1|  Headphones|             75.25|        79.99|      79.99|\n",
      "+-------+----------+-------------------+------------+------------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "         users.name\n",
    "        ,orders.order_date\n",
    "        ,order_items.quantity as order_item_quantity\n",
    "        ,products.name as product_name\n",
    "        ,orders.total_amount as order_total_amount\n",
    "        ,products.price as product_price\n",
    "        ,products.price * order_items.quantity total_price\n",
    "    from users\n",
    "    left join orders\n",
    "        on users.id = orders.user_id\n",
    "    left join order_items\n",
    "        on orders.id = order_items.order_id\n",
    "    left join products\n",
    "        on order_items.product_id = products.id\n",
    "    order by name, order_id\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5edb1d-b3ba-4c06-863f-0a59d394940d",
   "metadata": {},
   "source": [
    "### Data Analysis\r\n",
    "\r\n",
    "After performing the SQL joins, we obtained the output shown above. However, an inconsistency in the data was identified. The **total price of each product purchased by the user** is calculated by multiplying the `price` from the `products` table by the `quantity` from the `order_items` table. This calculation is shown in the `total_price` column.\r\n",
    "\r\n",
    "The inconsistency arises when comparing this calculated `total_price` to the `order_total_amount` column, which comes from the `orders` table. As you can see, the values in `order_total_amount` show **significant discrepancies** compared to the calculated `total_price` and the individual product prices. This suggests one of the following:\r\n",
    "\r\n",
    "1. **Incorrect Data Population**: The `order_total_amount` values in the `orders` table may have been incorrectly populated.\r\n",
    "2. **Different Meaning**: The `order_total_amount` might represent something other than the sum of product prices, such as:\r\n",
    "   - **Discounts**: The total amount after applying discounts.\r\n",
    "   - **Taxes**: The total amount including taxes or fees.\r\n",
    "   - **Historical Prices**: The product prices at the time of purchase, which may differ from the current prices in the `products` table.\r\n",
    "\r\n",
    "To resolve this inconsistency, further clarification is needed on what the `order_total_amount` column represents, and the data may need to be corrected or adjusted accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281709a5-b02a-4fe6-b915-8ba1a40f1681",
   "metadata": {},
   "source": [
    "Creating dataframe with query using the same joins used previously to link the order_items, orders, products, and users tables. However, this time we are also using the SUM function to calculate the total amount spent by each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4609972-6a0e-4ad7-bcbc-586cbf07cce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+\n",
      "|user_id|   name|total_spent|\n",
      "+-------+-------+-----------+\n",
      "|      1|  Alice|    1759.96|\n",
      "|      2|    Bob|      49.99|\n",
      "|      3|Charlie|     119.97|\n",
      "|      4|  David|    1999.98|\n",
      "|      5|    Eve|      79.99|\n",
      "+-------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_df = spark.sql(\"\"\"\n",
    "    select \n",
    "         users.id as user_id\n",
    "        ,users.name\n",
    "        ,sum(products.price * order_items.quantity) as total_spent\n",
    "    from order_items\n",
    "    left join orders\n",
    "        on order_items.order_id = orders.id\n",
    "    left join products\n",
    "        on products.id = order_items.product_id\n",
    "    left join users\n",
    "        on users.id = orders.user_id\n",
    "    group by users.name, users.id\n",
    "    order by name\n",
    "  \"\"\")\n",
    "\n",
    "results_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ee98b1-2057-4f00-9100-f0dafb31b70b",
   "metadata": {},
   "source": [
    "Writing the results from the `results_df` DataFrame to a MySQL database and showing the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ec4c382-a6b7-4517-a5cd-722830928491",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.write.jdbc(url=database_url, table=\"Results\", mode=\"overwrite\", properties=database_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e30214a-6262-45e2-8c92-90ef71ce9180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "| TABLE_NAME|\n",
      "+-----------+\n",
      "|    Results|\n",
      "|order_items|\n",
      "|     orders|\n",
      "|   products|\n",
      "|      users|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT TABLE_NAME \n",
    "    FROM tables_metadatas\n",
    "    WHERE TABLE_SCHEMA = 'test_db'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e08555be-c2f7-415a-a9d6-1183dddc2745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+\n",
      "|user_id|   name|total_spent|\n",
      "+-------+-------+-----------+\n",
      "|      1|  Alice|    1759.96|\n",
      "|      2|    Bob|      49.99|\n",
      "|      3|Charlie|     119.97|\n",
      "|      4|  David|    1999.98|\n",
      "|      5|    Eve|      79.99|\n",
      "+-------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_and_show_view(\"Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a9b37-a225-47d1-a307-119153e50039",
   "metadata": {},
   "source": [
    "Writing the `results_df` DataFrame to local disk as a Delta table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efdf5f18-13e9-45a6-8577-d214738174b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/06 12:35:54 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "delta_table_path = \"/home/jovyan/spark-delta-table\"\n",
    "results_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
